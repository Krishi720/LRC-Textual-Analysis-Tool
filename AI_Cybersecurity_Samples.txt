Human text on AI's role in Cybersecurity:
Machine learning is designed to train a computer to complete a certain task on its own. It is expected to change the cyberspace landscape in several ways.
First, new algorithms using machine learning are more adaptive, offering enhanced dynamism. Because cybersecurity risks evolve quickly over time, new generations of malware and cyberattacks are difficult to detect with traditional cybersecurity protocols. Machine learning overcomes this weakness by allowing cybersecurity systems to use pre-existing cyberattack data to respond to similar attacks.
Second, machine learning reduces the need for human labour in cybersecurity interactions, both in terms of offence and defence. A typical example of this would be spear-phishing, which tricks a specific individual or organization into leaking confidential information. Traditional methods of spear-phishing are often of limited scope. Effective intrusion requires a large amount of research on the potential target. Moreover, it is difficult, if not unfeasible, to attack multiple targets simultaneously. Yet, with the help of machine learning, automation of spear-phishing may be possible.
The third area in which machine learning may make a difference is attribution.
With more powerful learning capabilities, these algorithms are better able to locate critical evidence to reveal the real identity of an attacker, for example if certain code fragments mimic existing malware structures.
Despite its extensive use, the concept of cyber-deterrence remains unclear and under debate. There is disagreement over whether cyberattacks could be effectively deterred and even about whether the notion of deterrence is meaningful in cyberspace. However, according to those who are in favour of cyber-deterrence, it is feasible when certain conditions are met. For example, cyber-deterrence may work when it makes the cost of cyberattack exceptionally high. This can be achieved either by enhancing cyber-defence, thus enabling ‘deterrence by denial’, or by making retaliation credible and powerful, thus enabling ‘deterrence by punishment’. Another condition is that, unlike the traditional notion of deterrence, cyber-deterrence cannot be absolute. This means that some kinds of actor and some types of action cannot be deterred. Deterrence is most likely to work when attempting to deter cyberattacks that would have severe consequences and strategic purposes. Normally such attacks are planned and conducted by states.
A related condition is that the problem of attribution of an attack can be resolved to a degree when the strategic context and operational reality are taken into consideration. This leads to greater confidence that a cyberattack has been initiated by a state actor, as was the case with the use of the Stuxnet worm against Iranian nuclear facilities. The level of intelligence and technological capabilities required to carry out such an attack narrows down the list of suspects. In other words, attribution becomes less of a problem if the target of deterrence is a state, particularly a powerful one, and the behaviour to be deterred is a sophisticated, strategic cyberattack against an air-gapped facility. This has impli¬cations through¬ out civilian and military nuclear infrastructure.
Considering these conditions, the impact of machine learning on cyber-deterrence is ambiguous. On the one hand, cyber-deterrence may be enhanced. A typical example is in cyber-defence. By providing more active and adaptive defence, reducing human effort in monitoring threats and generating a timelier response, machine learning may raise the cost for a potential attacker and thus help promote deterrence by denial in cyberspace. Attribution is another area that machine learning may bolster. Deterrence would seem more credible if the attacker were to lose its anonymity. As such, cyber-deterrence could be more feasible with the intervention of machine learning.
Even if the above conditions are met, several factors may also reduce the effectiveness of cyber-deterrence.
The first is the possibility of adversarial machine learning. As cyber-defence models based on machine learning become more effective at detecting threats, potential attackers may look for ways to confuse the models. This is often called adversarial machine learning. Even if actors on the defensive side can rely on AI models to safeguard their systems, their confidence in deterrence by denial must not be exaggerated. This is because offenders may succeed in poisoning the models (also known as machine learning poisoning) or may find other ways to evade them. In this sense, the security benefits offered by AI could be offset. However, a 2018 report warned that relatively little attention has been paid to making AI-based defences robust against attackers that anticipate their use.5 Ironically, the use of machine learning for cyber-defence can actually expand the attack surface of a defence system—the points at which an attacker can interact with the system— due to this lack of attention and other vulnerabilities.
A second problem is the blurred connection between actors and capabilities. Strategic cyberattacks—attacks that inflict damage of strategic national impact— are currently unlikely to be conducted by individuals. Cyber operations that intend to change the target’s behaviour or to make the target bear considerable losses often involve complex efforts for preparation, organization, coordination, and testing and rehearsal. They necessitate an abundance of critical resources, such as discovery of zero-day vulnerabilities, hacking tools and talent. Such cyber operations also require adequate information about the target systems’ defence prepared¬ness. However, with the development of machine learning, these efforts could be performed or facilitated by automated and adaptive programmes. These pro¬grammes would be able to dig up vulnerabilities, circumvent detection, defeat anti-malware systems or even redesign an operation according to the recognized properties of the target system. This means that complex cyber operations may not require the same level of organizational complexity in the AI era. Individual hackers or small groups could also complete tasks that have strategic consequences. This would create several knock-on problems for cyber-deterrence. First, the ‘known identity plus known demand’ condition would be more difficult to establish—that is, being able to determine both the source of the attack and its aims would be muddied.6 Second, attribution would become more difficult because capabilities, including other forensic evidence such as language or similarity with past operations, may no longer be a reliable indicator for attribution. Third, cyberattacks with severe consequences may proliferate, undermining strategic stability in cyberspace.
Finally, increased use of AI in cybersecurity would make the gathering and sharing of data even more important. This may make alliance politics in cyberspace more common and intensive. Machine learning-based cyber-defence normally takes two forms. One is supervised learning, where the goal is to learn from known threats and to generalize and apply this knowledge to new threats. The other is unsupervised learning, in which programmes try to find suspicious deviations from normal behaviour. Either form would require extensive analysis of data and strong intelligence capabilities and networks. Therefore, to make deterrence effective, a state would need to cooperate with other states in information sharing in order to build a global intelligence network. This may encourage alliance relationships in cyberspace. The negative outcome would be an intensification of the already evident cleavages in cyberspace, creating a sense of antagonism between different groups and making global consensus on cybersecurity norms even harder to reach.
Overall, AI and machine learning pose risks and offer benefits to cybersecurity. The impact on cyber-deterrence remains unclear, since both defence and offence could be buttressed by the development of AI. This suggests the need for more dialogue among AI researchers, strategic researchers, policymakers and other relevant stakeholders to reach greater clarity on cyber-deterrence and how it may have an impact on future strategic relations and arsenals.

GPT text on AI's role in Cybersecurity:
Artificial intelligence is reshaping both the conduct of operations in cyberspace and the theory and practice of deterrence that seeks to contain them. Key terms framing this analysis include machine learning, deterrence by denial and by punishment, attribution, signaling, credibility, escalation, adversarial examples, data poisoning, anomaly detection, predictive analytics, deception, explainability, robustness, human-in-the-loop, red lines, and confidence-building measures. The accelerating integration of these concepts raises the prospect of faster, more adaptive defense and offense, while also introducing fresh instability rooted in speed, opacity, and error propagation. Understanding how AI alters incentives, information, and perceptions is therefore central to evaluating its role in cyber-deterrence.
Machine learning already permeates network defense through anomaly detection, malware classification, user and entity behavior analytics, and automated incident triage. These capabilities can strengthen deterrence by denial: if would-be attackers expect to face resilient systems that detect, contain, and recover rapidly, the expected payoff of intrusion declines. Supervised and unsupervised models trained on large threat-intelligence corpora—tactics, techniques, and procedures codified in frameworks like ATT\&CK—compress the time from detection to response. Reinforcement-learning agents can recommend containment actions; generative models can synthesize signatures and hypotheses about variants of known threats. In aggregate, such tools raise the perceived costs of success for adversaries, and—if the defender can signal these capabilities credibly—alter attacker calculus before operations begin.
At the same time, AI increases the adaptability and scale of offense. Automated reconnaissance, vulnerability discovery, and spear-phishing driven by large language models reduce the marginal cost of targeting many victims, while code-generation systems can help adversaries weaponize proof-of-concept exploits faster. Deepfake-enabled social engineering, coupled with real-time text-to-speech and video fabrication, erodes traditional trust anchors inside organizations. From a deterrence perspective, this dual-use character of AI complicates signaling. Demonstrating defensive competence without revealing exploitable model artifacts is challenging; showcasing offensive prowess to deter through punishment risks normalizing escalatory tools and triggering arms-race dynamics. The same modularity that makes AI powerful also makes it portable, compressing proliferation timelines and widening the set of actors capable of sophisticated operations.
Attribution occupies a pivotal place in classical deterrence theory and is likewise transformed by AI. On the one hand, machine learning can correlate code reuse, infrastructure fingerprints, operational tempos, and linguistic features across incidents to strengthen probabilistic attribution. Multimodal fusion—combining network telemetry, malware genealogy, and open-source signals—can raise confidence intervals and reduce decision time. On the other hand, adversarial manipulation threatens to contaminate the very data on which such inferences rely. False-flag indicators deliberately inserted into toolchains, synthetic log events, and model-targeted evasion (adversarial examples) can skew classifiers away from true perpetrators. The resulting attribution uncertainty undermines both punishment and the credibility of declaratory policy. If states fear misattribution, they may self-deter; if they over-trust opaque models, they may retaliate on faulty grounds.
Conditions for effective deterrence in the AI-suffused cyber domain therefore revolve around credibility, capability, communication, and control. Credibility depends not only on having AI-enabled defensive and retaliatory options, but also on demonstrably integrating them into doctrine, exercises, and procurement. Capability must be robust in the face of adaptive adversaries: models require continuous retraining pipelines, rigorous red-teaming, and defenses against data poisoning and model inversion. Communication concerns both external signaling and internal governance. Externally, states must articulate thresholds, expected responses, and partnerships with critical infrastructure operators in language sufficiently specific to shape expectations yet flexible enough to accommodate uncertainty. Internally, they must ensure human-in-the-loop or human-on-the-loop arrangements, audit trails, and post-hoc explainability adequate for accountability. Control, finally, demands guardrails that prevent automation surprises: escalation management requires pacing functions and fail-safes for time-critical responses.
These conditions face several frictions. Speed is the first. AI compresses the decision cycle on both sides, enabling autonomous or semi-autonomous playbooks to act at machine timescales. While this can blunt fast-moving threats, it also shortens the window for human judgment and diplomatic de-escalation. Second is opacity. Complex models hinder explainability, making it harder to justify actions to domestic and international audiences. Third is brittleness. Many high-performing models degrade under distribution shift—when attackers alter payloads or network baselines—or when confronted with deliberate perturbations. Fourth is entanglement. Cyber operations intersect with economic statecraft, information operations, and even the command, control, and communications that underpin conventional deterrence. Misinterpreted signals in one layer can reverberate across others, especially if AI systems amplify noisy indicators into confident (but wrong) assessments.
Deterrence by denial remains the most promising axis for leveraging AI constructively. Investing in predictive maintenance for security controls, continuous authentication, micro-segmentation, and moving-target defense can make intrusions costly and persistent presence difficult. AI can power adaptive deception—honeypots that morph in response to attacker behavior, synthetic environments that waste adversary resources, and canaries embedded in data at rest. These measures do not require public demonstrations of punitive capability; rather, they quietly change the economics of intrusion. However, to contribute to deterrence, even denial benefits from selective transparency. Periodic metrics on mean time to detect and recover, independent audits of model robustness, and participation in joint exercises with industry can signal resilience without exposing sensitive tradecraft.
Deterrence by punishment in cyberspace has always struggled with proportionality, temporality, and verifiability. AI sharpens these challenges. Calibrating a punitive response informed by model-generated attribution may invite criticism if the evidentiary basis cannot be shared. Moreover, the reversibility of many cyber effects complicates the translation of damage into punishment narratives, while cross-domain options (economic sanctions, legal indictments, kinetic demonstrations) carry their own escalatory ladders. To preserve the option value of punishment while reducing instability, states may emphasize cumulative strategies: persistent engagement below thresholds that, over time, degrade adversary infrastructure and tradecraft; legal and financial pressure on enabling ecosystems; and collaborative takedowns with platforms and providers. Even here, AI must be disciplined: content moderation and automated takedowns used against botnets or coordinated inauthentic behavior should be auditable to avoid collateral suppression of legitimate activity.
The data substrate of AI demands special attention. Training corpora for security models can inadvertently encode sensitive network patterns; if exfiltrated, they become reconnaissance goldmines. Poisoning risks are not merely theoretical: an attacker who subtly seeds mislabeled or biased samples into public repositories or threat-sharing feeds can nudge models toward systematic blind spots. Supply-chain security—model provenance, dependency integrity, signed datasets, and reproducible pipelines—consequently becomes part of deterrence posture. So too does privacy protection: deploying pervasive telemetry for anomaly detection must be reconciled with civil liberties and data-minimization obligations. Domestic legitimacy is not a cosmetic concern; it is integral to the credibility of external signaling in democratic societies.
Because signaling is central to deterrence, the problem of communicating about AI systems that are inherently complex and dynamic looms large. States can mitigate this by investing in narrative-ready artifacts: doctrine that explains roles and limits of automation; case studies from exercises; model cards and system cards that summarize capabilities and constraints; and third-party attestations that systems meet specified robustness benchmarks. Multilateral venues can help standardize taxonomies for severity, thresholds, and evidence, reducing the risk that ambiguity becomes a bargaining chip for opportunistic escalation. Confidence-building measures suited to AI—shared red-team scenarios, incident-response hotlines augmented with automated triage but staffed by accountable humans, and norms on non-interference with each other’s election infrastructure or hospitals—translate abstract reassurance into concrete practice.
None of this obviates the adversarial co-evolution inherent in cybersecurity. As defenders adopt AI to raise the bar, attackers will test and repurpose the same tools. The determinant of long-run deterrence effectiveness will be organizational learning: the ability to integrate empirical feedback into models, to sunset brittle capabilities, to rotate keys—both cryptographic and conceptual—before adversaries fully adapt. Institutions that blend engineering excellence with strategic judgment will fare best. Education and workforce development thus belong inside the deterrence discussion: analysts capable of interrogating model outputs, policy professionals literate in the failure modes of AI, and leaders comfortable making decisions under probabilistic ambiguity are as crucial as GPUs and datasets.
Ultimately, AI’s role in cyber-deterrence is neither straightforwardly stabilizing nor inherently destabilizing. It is contingent on design choices, governance, and the international political environment into which it is introduced. The near-term path of least regret prioritizes deterrence by denial, systematic robustness, and disciplined signaling, while treating punishment options as carefully husbanded, evidence-anchored instruments. Because technologies and tradecraft are evolving faster than legal and diplomatic frameworks, sustained interaction among states, industry, researchers, and civil society is indispensable. Track-two dialogues, technical standards that encode guardrails, and repeatable joint exercises can build the shared mental models required to prevent miscalculation. The central insight is modest but urgent: only by aligning technical capability with transparent doctrine and accountable control can AI strengthen cyber-deterrence rather than erode it, and only through deliberate engagement can consensus keep pace with code.
